{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b34d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sample  gene_0    gene_1    gene_2    gene_3     gene_4  gene_5  \\\n",
      "0      sample_0     0.0  2.017209  3.265527  5.478487  10.431999     0.0   \n",
      "1      sample_1     0.0  0.592732  1.588421  7.586157   9.623011     0.0   \n",
      "2      sample_2     0.0  3.511759  4.327199  6.881787   9.870730     0.0   \n",
      "3      sample_3     0.0  3.663618  4.507649  6.659068  10.196184     0.0   \n",
      "4      sample_4     0.0  2.655741  2.821547  6.539454   9.738265     0.0   \n",
      "..          ...     ...       ...       ...       ...        ...     ...   \n",
      "796  sample_796     0.0  1.865642  2.718197  7.350099  10.006003     0.0   \n",
      "797  sample_797     0.0  3.942955  4.453807  6.346597  10.056868     0.0   \n",
      "798  sample_798     0.0  3.249582  3.707492  8.185901   9.504082     0.0   \n",
      "799  sample_799     0.0  2.590339  2.787976  7.318624   9.987136     0.0   \n",
      "800  sample_800     0.0  2.325242  3.805932  6.530246   9.560367     0.0   \n",
      "\n",
      "       gene_6    gene_7  gene_8  ...  gene_20522  gene_20523  gene_20524  \\\n",
      "0    7.175175  0.591871     0.0  ...    8.210257    9.723516    7.220030   \n",
      "1    6.816049  0.000000     0.0  ...    7.323865    9.740931    6.256586   \n",
      "2    6.972130  0.452595     0.0  ...    8.127123   10.908640    5.401607   \n",
      "3    7.843375  0.434882     0.0  ...    8.792959   10.141520    8.942805   \n",
      "4    6.566967  0.360982     0.0  ...    8.891425   10.373790    7.181162   \n",
      "..        ...       ...     ...  ...         ...         ...         ...   \n",
      "796  6.764792  0.496922     0.0  ...    9.118313   10.004852    4.484415   \n",
      "797  7.320331  0.000000     0.0  ...    9.623335    9.823921    6.555327   \n",
      "798  7.536589  1.811101     0.0  ...    8.610704   10.485517    3.589763   \n",
      "799  9.213464  0.000000     0.0  ...    8.605387   11.004677    4.745888   \n",
      "800  7.957027  0.000000     0.0  ...    8.594354   10.243079    9.139459   \n",
      "\n",
      "     gene_20525  gene_20526  gene_20527  gene_20528  gene_20529  gene_20530  \\\n",
      "0      9.119813   12.003135    9.650743    8.921326    5.286759    0.000000   \n",
      "1      8.381612   12.674552   10.517059    9.397854    2.094168    0.000000   \n",
      "2      9.911597    9.045255    9.788359   10.090470    1.683023    0.000000   \n",
      "3      9.601208   11.392682    9.694814    9.684365    3.292001    0.000000   \n",
      "4      9.846910   11.922439    9.217749    9.461191    5.110372    0.000000   \n",
      "..          ...         ...         ...         ...         ...         ...   \n",
      "796    9.614701   12.031267    9.813063   10.092770    8.819269    0.000000   \n",
      "797    9.064002   11.633422   10.317266    8.745983    9.659081    0.000000   \n",
      "798    9.350636   12.180944   10.681194    9.466711    4.677458    0.586693   \n",
      "799    9.626383   11.198279   10.335513   10.400581    5.718751    0.000000   \n",
      "800   10.102934   11.641081   10.607358    9.844794    4.550716    0.000000   \n",
      "\n",
      "     Class  \n",
      "0     PRAD  \n",
      "1     LUAD  \n",
      "2     PRAD  \n",
      "3     PRAD  \n",
      "4     BRCA  \n",
      "..     ...  \n",
      "796   BRCA  \n",
      "797   LUAD  \n",
      "798   COAD  \n",
      "799   PRAD  \n",
      "800   PRAD  \n",
      "\n",
      "[801 rows x 20533 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file\n",
    "df1 = pd.read_csv('se_data.csv')\n",
    "\n",
    "# Load the second CSV file\n",
    "df2 = pd.read_csv('se_labels.csv')\n",
    "df2.rename(columns={'Unnamed: 0': 'sample'}, inplace=True)\n",
    "df1.rename(columns={'Unnamed: 0': 'sample'}, inplace=True)\n",
    "# Merge the two datasets on the 'sample' column\n",
    "df = pd.merge(df1, df2, on='sample', how='inner')\n",
    "]\n",
    "\n",
    "\n",
    "# Print the merged dataset\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce31ad5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "sample        0\n",
      "gene_0        0\n",
      "gene_1        0\n",
      "gene_2        0\n",
      "gene_3        0\n",
      "             ..\n",
      "gene_20527    0\n",
      "gene_20528    0\n",
      "gene_20529    0\n",
      "gene_20530    0\n",
      "Class         0\n",
      "Length: 20533, dtype: int64\n",
      "\n",
      "Duplicate samples:\n",
      "Empty DataFrame\n",
      "Columns: [sample, gene_0, gene_1, gene_2, gene_3, gene_4, gene_5, gene_6, gene_7, gene_8, gene_9, gene_10, gene_11, gene_12, gene_13, gene_14, gene_15, gene_16, gene_17, gene_18, gene_19, gene_20, gene_21, gene_22, gene_23, gene_24, gene_25, gene_26, gene_27, gene_28, gene_29, gene_30, gene_31, gene_32, gene_33, gene_34, gene_35, gene_36, gene_37, gene_38, gene_39, gene_40, gene_41, gene_42, gene_43, gene_44, gene_45, gene_46, gene_47, gene_48, gene_49, gene_50, gene_51, gene_52, gene_53, gene_54, gene_55, gene_56, gene_57, gene_58, gene_59, gene_60, gene_61, gene_62, gene_63, gene_64, gene_65, gene_66, gene_67, gene_68, gene_69, gene_70, gene_71, gene_72, gene_73, gene_74, gene_75, gene_76, gene_77, gene_78, gene_79, gene_80, gene_81, gene_82, gene_83, gene_84, gene_85, gene_86, gene_87, gene_88, gene_89, gene_90, gene_91, gene_92, gene_93, gene_94, gene_95, gene_96, gene_97, gene_98, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 20533 columns]\n",
      "\n",
      "After removing duplicates:\n",
      "         sample  gene_0    gene_1    gene_2    gene_3     gene_4  gene_5  \\\n",
      "0      sample_0     0.0  2.017209  3.265527  5.478487  10.431999     0.0   \n",
      "1      sample_1     0.0  0.592732  1.588421  7.586157   9.623011     0.0   \n",
      "2      sample_2     0.0  3.511759  4.327199  6.881787   9.870730     0.0   \n",
      "3      sample_3     0.0  3.663618  4.507649  6.659068  10.196184     0.0   \n",
      "4      sample_4     0.0  2.655741  2.821547  6.539454   9.738265     0.0   \n",
      "..          ...     ...       ...       ...       ...        ...     ...   \n",
      "796  sample_796     0.0  1.865642  2.718197  7.350099  10.006003     0.0   \n",
      "797  sample_797     0.0  3.942955  4.453807  6.346597  10.056868     0.0   \n",
      "798  sample_798     0.0  3.249582  3.707492  8.185901   9.504082     0.0   \n",
      "799  sample_799     0.0  2.590339  2.787976  7.318624   9.987136     0.0   \n",
      "800  sample_800     0.0  2.325242  3.805932  6.530246   9.560367     0.0   \n",
      "\n",
      "       gene_6    gene_7  gene_8  ...  gene_20522  gene_20523  gene_20524  \\\n",
      "0    7.175175  0.591871     0.0  ...    8.210257    9.723516    7.220030   \n",
      "1    6.816049  0.000000     0.0  ...    7.323865    9.740931    6.256586   \n",
      "2    6.972130  0.452595     0.0  ...    8.127123   10.908640    5.401607   \n",
      "3    7.843375  0.434882     0.0  ...    8.792959   10.141520    8.942805   \n",
      "4    6.566967  0.360982     0.0  ...    8.891425   10.373790    7.181162   \n",
      "..        ...       ...     ...  ...         ...         ...         ...   \n",
      "796  6.764792  0.496922     0.0  ...    9.118313   10.004852    4.484415   \n",
      "797  7.320331  0.000000     0.0  ...    9.623335    9.823921    6.555327   \n",
      "798  7.536589  1.811101     0.0  ...    8.610704   10.485517    3.589763   \n",
      "799  9.213464  0.000000     0.0  ...    8.605387   11.004677    4.745888   \n",
      "800  7.957027  0.000000     0.0  ...    8.594354   10.243079    9.139459   \n",
      "\n",
      "     gene_20525  gene_20526  gene_20527  gene_20528  gene_20529  gene_20530  \\\n",
      "0      9.119813   12.003135    9.650743    8.921326    5.286759    0.000000   \n",
      "1      8.381612   12.674552   10.517059    9.397854    2.094168    0.000000   \n",
      "2      9.911597    9.045255    9.788359   10.090470    1.683023    0.000000   \n",
      "3      9.601208   11.392682    9.694814    9.684365    3.292001    0.000000   \n",
      "4      9.846910   11.922439    9.217749    9.461191    5.110372    0.000000   \n",
      "..          ...         ...         ...         ...         ...         ...   \n",
      "796    9.614701   12.031267    9.813063   10.092770    8.819269    0.000000   \n",
      "797    9.064002   11.633422   10.317266    8.745983    9.659081    0.000000   \n",
      "798    9.350636   12.180944   10.681194    9.466711    4.677458    0.586693   \n",
      "799    9.626383   11.198279   10.335513   10.400581    5.718751    0.000000   \n",
      "800   10.102934   11.641081   10.607358    9.844794    4.550716    0.000000   \n",
      "\n",
      "     Class  \n",
      "0     PRAD  \n",
      "1     LUAD  \n",
      "2     PRAD  \n",
      "3     PRAD  \n",
      "4     BRCA  \n",
      "..     ...  \n",
      "796   BRCA  \n",
      "797   LUAD  \n",
      "798   COAD  \n",
      "799   PRAD  \n",
      "800   PRAD  \n",
      "\n",
      "[801 rows x 20533 columns]\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Check for duplicate samples\n",
    "duplicate_samples = df.duplicated(subset='sample', keep='first')\n",
    "print(\"\\nDuplicate samples:\")\n",
    "print(df[duplicate_samples])\n",
    "\n",
    "# Remove duplicate samples\n",
    "df.drop_duplicates(subset='sample', keep='first', inplace=True)\n",
    "\n",
    "print(\"\\nAfter removing duplicates:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d790b09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "  # Assume 'merged_data.csv' is your cleaned and merged file\n",
    "\n",
    "# Separating features and target\n",
    "X = df.drop(columns=['sample', 'Class'])  # Drop non-feature columns\n",
    "y = df['Class']  # Target variable\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Save the scaled features and target into a new DataFrame\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "df_scaled['Class'] = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0538613",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "    # Assuming X_scaled is already defined and scaled\n",
    "    A = kneighbors_graph(X_scaled, n_neighbors=5, mode='connectivity', include_self=True)\n",
    "\n",
    "    # Convert to numpy array first\n",
    "    edges = np.array(A.nonzero())\n",
    "\n",
    "    # Now convert the numpy array to a PyTorch tensor\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35208ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "num_nodes = data.num_nodes\n",
    "train_size = int(num_nodes * 0.8)  # 80% for training\n",
    "val_size = num_nodes - train_size  # Remaining for validation\n",
    "\n",
    "# Randomly generate a permutation of node indices\n",
    "perm = torch.randperm(num_nodes)\n",
    "\n",
    "# Create train and validation masks\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[perm[:train_size]] = True\n",
    "val_mask[perm[train_size:]] = True\n",
    "\n",
    "# Add masks to data object\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a828c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1fb26ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\vrkve\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\vrkve\\anaconda3\\lib\\site-packages (from scikit-learn) (1.19.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\vrkve\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\vrkve\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vrkve\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'c:\\\\users\\\\vrkve\\\\anaconda3\\\\lib\\\\site-packages\\\\scipy-1.13.0.dist-info\\\\METADATA'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96b933e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Using cached imbalanced_learn-0.12.2-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\vrkve\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.19.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\vrkve\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.13.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\vrkve\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.2.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\vrkve\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\vrkve\\anaconda3\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'c:\\\\users\\\\vrkve\\\\anaconda3\\\\lib\\\\site-packages\\\\scipy-1.13.0.dist-info\\\\METADATA'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82a2a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Train Loss: 6.7418, Train Accuracy: 54.17%, Val Loss: 0.4222, Val Accuracy: 93.12%\n",
      "Epoch 2/25, Train Loss: 0.1608, Train Accuracy: 96.25%, Val Loss: 0.1507, Val Accuracy: 96.25%\n",
      "Epoch 3/25, Train Loss: 0.0004, Train Accuracy: 100.00%, Val Loss: 0.1358, Val Accuracy: 96.88%\n",
      "Epoch 4/25, Train Loss: 0.0004, Train Accuracy: 100.00%, Val Loss: 0.1295, Val Accuracy: 96.88%\n",
      "Epoch 5/25, Train Loss: 0.0001, Train Accuracy: 100.00%, Val Loss: 0.1102, Val Accuracy: 98.12%\n",
      "Epoch 6/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.1026, Val Accuracy: 98.12%\n",
      "Epoch 7/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0968, Val Accuracy: 98.12%\n",
      "Epoch 8/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0929, Val Accuracy: 98.12%\n",
      "Epoch 9/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0891, Val Accuracy: 98.12%\n",
      "Epoch 10/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0854, Val Accuracy: 98.12%\n",
      "Epoch 11/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0827, Val Accuracy: 98.12%\n",
      "Epoch 12/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0792, Val Accuracy: 98.12%\n",
      "Epoch 13/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0763, Val Accuracy: 98.12%\n",
      "Epoch 14/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0730, Val Accuracy: 98.75%\n",
      "Epoch 15/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0709, Val Accuracy: 99.38%\n",
      "Epoch 16/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0688, Val Accuracy: 99.38%\n",
      "Epoch 17/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0670, Val Accuracy: 99.38%\n",
      "Epoch 18/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0653, Val Accuracy: 99.38%\n",
      "Epoch 19/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0638, Val Accuracy: 99.38%\n",
      "Epoch 20/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0623, Val Accuracy: 99.38%\n",
      "Epoch 21/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0613, Val Accuracy: 99.38%\n",
      "Epoch 22/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0600, Val Accuracy: 99.38%\n",
      "Epoch 23/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0590, Val Accuracy: 99.38%\n",
      "Epoch 24/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0580, Val Accuracy: 99.38%\n",
      "Epoch 25/25, Train Loss: 0.0000, Train Accuracy: 100.00%, Val Loss: 0.0572, Val Accuracy: 99.38%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        BRCA       0.97      1.00      0.98        60\n",
      "        COAD       1.00      0.94      0.97        16\n",
      "        KIRC       1.00      0.97      0.98        30\n",
      "        LUAD       0.96      0.93      0.95        28\n",
      "        PRAD       0.96      1.00      0.98        27\n",
      "\n",
      "    accuracy                           0.98       161\n",
      "   macro avg       0.98      0.97      0.97       161\n",
      "weighted avg       0.98      0.98      0.97       161\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load data\n",
    "df1 = pd.read_csv('se_data.csv')\n",
    "df2 = pd.read_csv('se_labels.csv')\n",
    "df1.rename(columns={'Unnamed: 0': 'sample'}, inplace=True)\n",
    "df2.rename(columns={'Unnamed: 0': 'sample'}, inplace=True)\n",
    "df = pd.merge(df1, df2, on='sample', how='inner')\n",
    "\n",
    "# Handling missing values and duplicates\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(subset='sample', inplace=True)\n",
    "\n",
    "# Feature Scaling\n",
    "X = df.drop(columns=['sample', 'Class']).values\n",
    "y = df['Class'].values\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Splitting data into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train[:, None, :], dtype=torch.float32)  # Add channel dimension\n",
    "X_val_tensor = torch.tensor(X_val[:, None, :], dtype=torch.float32)  # Add channel dimension\n",
    "X_test_tensor = torch.tensor(X_test[:, None, :], dtype=torch.float32)  # Add channel dimension\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# Define the 1D-CNN model\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.layer1 = nn.Conv1d(1, 16, kernel_size=3, padding=1)\n",
    "        self.layer2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        # Calculate the number of features after two pooling layers\n",
    "        num_pooled_features = num_features // 4  # adjust based on your pooling layers\n",
    "        self.fc1 = nn.Linear(32 * num_pooled_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model and move to device\n",
    "num_features = X_train.shape[1]  # Number of features for each sample\n",
    "num_classes = len(np.unique(y_encoded))  # Number of output classes\n",
    "model = CNN1D(num_features, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "# Train the model\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_accuracy = 100 * correct_train / total_train\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = 100 * correct_val / total_val\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2ca325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
